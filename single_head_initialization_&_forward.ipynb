{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Head Initialization\n",
    "In a Transformer, each token is passed through the model as a vector. The `hidden_size` defines how wide the token vector is when it reaches the Attention mechanism.\n",
    "\n",
    "Our Attention layers will allow disabling bias terms for linear layers since recent papers and models, such as Cramming, Pythia, and PaLM, have shown that disabling the bias term results in little-to-no downstream performance drop while decreasing computational and memory requirements. However, to match the original Transformer implementation, we will set it on by default.\n",
    "\n",
    "In this implementation, we will start merge Wa, WK 5, and WY into single linear layer, Wakv, and unbind the outputs into Q, K, and V. \n",
    "\n",
    "This is accomplished by increasing the output shape by a factor of three. This is mathematically equivalent to three individual linear layers, each with the same input and output shape.\n",
    "\n",
    "In Multi-Head Attention, each individual head size is smaller than the input size?, so for Single Head we will arbitrarily set the head size to be four times smaller than the input dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleHeadAttention(nn.Module):\n",
    "    def __init__(self, hidden_size: int, bias: bool = True):\n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
